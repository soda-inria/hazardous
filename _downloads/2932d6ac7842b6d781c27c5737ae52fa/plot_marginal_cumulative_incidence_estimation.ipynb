{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Estimating marginal cumulative incidence functions\n\nThis example demonstrates how to estimate the marginal cumulative incidence\nusing :class:`hazardous.GradientBoostingIncidence` and compares the results to\nthe Aalen-Johansen estimator and to the theoretical cumulated incidence curves\non synthetic data.\n\nHere the data is generated by taking the minimum time of samples from three\ncompeting Weibull distributions with fixed parameters and without any\nconditioning covariate. In this case, the Aalen-Johansen estimator is expected\nto be unbiased, and this is empirically confirmed by this example.\n\nThe :class:`hazardous.GradientBoostingIncidence` estimator on the other hand is\na predictive estimator that expects at least one conditioning covariate. In\nthis example, we use a dummy covariate that is constant for all samples. Here\nwe are not interested in the discrimination power of the estimator: there is\nnone by construction, since we do not have access to informative covariates.\nInstead we empirically study its marginal calibration, that is, its ability to\napproximately recover an unbiased estimate of the marginal cumulative incidence\nfunction for each competing event.\n\nThis example also highlights that :class:`hazardous.GradientBoostingIncidence`\nestimates noisy cumulative incidence functions, which are not smooth and not\neven monotonically increasing. This is a known limitation of the estimator,\nand attempting to enforce monotonicity at training time typically introduces\nsevere over-estimation bias for large time horizons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from time import perf_counter\nimport numpy as np\nfrom scipy.stats import weibull_min\nfrom sklearn.base import clone\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom hazardous import GradientBoostingIncidence\nfrom lifelines import AalenJohansenFitter\n\nrng = np.random.default_rng(0)\nn_samples = 3_000\n\n# Non-informative covariate because scikit-learn estimators expect at least\n# one feature.\nX_dummy = np.zeros(shape=(n_samples, 1), dtype=np.float32)\n\nbase_scale = 1_000.0  # some arbitrary time unit\n\ndistributions = [\n    {\"event_id\": 1, \"scale\": 10 * base_scale, \"shape\": 0.5},\n    {\"event_id\": 2, \"scale\": 3 * base_scale, \"shape\": 1},\n    {\"event_id\": 3, \"scale\": 3 * base_scale, \"shape\": 5},\n]\nevent_times = np.concatenate(\n    [\n        weibull_min.rvs(\n            dist[\"shape\"],\n            scale=dist[\"scale\"],\n            size=n_samples,\n            random_state=rng,\n        ).reshape(-1, 1)\n        for dist in distributions\n    ],\n    axis=1,\n)\nfirst_event_idx = np.argmin(event_times, axis=1)\n\ny_uncensored = pd.DataFrame(\n    dict(\n        event=first_event_idx + 1,  # 0 is reserved as the censoring marker\n        duration=event_times[np.arange(n_samples), first_event_idx],\n    )\n)\ny_uncensored[\"event\"].value_counts().sort_index()\nt_max = y_uncensored[\"duration\"].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we know the true distribution of the data, we can compute the\ntheoretical cumulative incidence functions (CIFs) by integrating the hazard\nfunctions. The CIFs are the probability of experiencing the event of interest\nbefore time t, given that the subject has not experienced any other event\nbefore time t.\n\nThe following function computes the hazard function of a [Weibull\ndistribution](https://en.wikipedia.org/wiki/Weibull_distribution):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def weibull_hazard(t, shape=1.0, scale=1.0, **ignored_kwargs):\n    # Plug an arbitrary finite hazard value at t==0 because fractional powers\n    # of 0 are undefined.\n    #\n    # XXX: this does not seem correct but in practice it does make it possible\n    # to integrate the hazard function into cumulative incidence functions in a\n    # way that matches the Aalen Johansen estimator.\n    with np.errstate(divide=\"ignore\"):\n        return np.where(t == 0, 0.0, (shape / scale) * (t / scale) ** (shape - 1.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that true CIFs are independent of the censoring distribution. We can use\nthem as reference to check that the estimators are unbiased by the censoring.\nHere are the two estimators of interest:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calculate_variance = n_samples <= 5_000\naj = AalenJohansenFitter(calculate_variance=calculate_variance, seed=0)\n\ngb_incidence = GradientBoostingIncidence(\n    learning_rate=0.03,\n    n_iter=100,\n    max_leaf_nodes=5,\n    hard_zero_fraction=0.1,\n    min_samples_leaf=50,\n    loss=\"ibs\",\n    show_progressbar=False,\n    random_state=0,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CIFs estimated on uncensored data\n\nLet's now estimate the CIFs on uncensored data and plot them against the\ntheoretical CIFs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_cumulative_incidence_functions(distributions, y, gb_incidence=None, aj=None):\n    _, axes = plt.subplots(figsize=(12, 4), ncols=len(distributions), sharey=True)\n\n    # Compute the estimate of the CIFs on a coarse grid.\n    coarse_timegrid = np.linspace(0, t_max, num=100)\n\n    # Compute the theoretical CIFs by integrating the hazard functions on a\n    # fine-grained time grid. Note that integration errors can accumulate quite\n    # quickly if the time grid is resolution too coarse, especially for the\n    # Weibull distribution with shape < 1.\n    tic = perf_counter()\n    fine_time_grid = np.linspace(0, t_max, num=10_000_000)\n    dt = np.diff(fine_time_grid)[0]\n    all_hazards = np.stack(\n        [weibull_hazard(fine_time_grid, **dist) for dist in distributions],\n        axis=0,\n    )\n    any_event_hazards = all_hazards.sum(axis=0)\n    any_event_survival = np.exp(-(any_event_hazards.cumsum(axis=-1) * dt))\n    print(\n        \"Integrated theoretical any event survival curve in\"\n        f\" {perf_counter() - tic:.3f} s\"\n    )\n\n    censoring_fraction = (y[\"event\"] == 0).mean()\n    plt.suptitle(\n        \"Cause-specific cumulative incidence functions\"\n        f\" ({censoring_fraction:.1%} censoring)\"\n    )\n\n    for event_id, (ax, hazards_i) in enumerate(zip(axes, all_hazards), 1):\n        theoretical_cif = (hazards_i * any_event_survival).cumsum(axis=-1) * dt\n        print(\n            \"Integrated theoretical cumulative incidence curve for event\"\n            f\" {event_id} in {perf_counter() - tic:.3f} s\"\n        )\n        downsampling_rate = fine_time_grid.size // coarse_timegrid.size\n        ax.plot(\n            fine_time_grid[::downsampling_rate],\n            theoretical_cif[::downsampling_rate],\n            linestyle=\"dashed\",\n            label=\"Theoretical incidence\",\n        ),\n\n        if gb_incidence is not None:\n            tic = perf_counter()\n            gb_incidence.set_params(event_of_interest=event_id)\n            gb_incidence.fit(X_dummy, y)\n            duration = perf_counter() - tic\n            print(f\"GB Incidence for event {event_id} fit in {duration:.3f} s\")\n            tic = perf_counter()\n            cif_pred = gb_incidence.predict_cumulative_incidence(\n                X_dummy[0:1], coarse_timegrid\n            )[0]\n            duration = perf_counter() - tic\n            print(f\"GB Incidence for event {event_id} prediction in {duration:.3f} s\")\n            ax.plot(\n                coarse_timegrid,\n                cif_pred,\n                label=\"GradientBoostingIncidence\",\n            )\n            ax.set(title=f\"Event {event_id}\")\n\n        if aj is not None:\n            tic = perf_counter()\n            aj.fit(y[\"duration\"], y[\"event\"], event_of_interest=event_id)\n            duration = perf_counter() - tic\n            print(f\"Aalen-Johansen for event {event_id} fit in {duration:.3f} s\")\n            aj.plot(label=\"Aalen-Johansen\", ax=ax)\n\n        if event_id == 1:\n            ax.legend(loc=\"lower right\")\n        else:\n            ax.legend().remove()\n\n\nplot_cumulative_incidence_functions(\n    distributions, gb_incidence=gb_incidence, aj=aj, y=y_uncensored\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CIFs estimated on censored data\n\nAdd some independent censoring with some arbitrary parameters to control the\namount of censoring: lowering the expected value bound increases the amount\nof censoring.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "censoring_times = weibull_min.rvs(\n    1.0,\n    scale=1.5 * base_scale,\n    size=n_samples,\n    random_state=rng,\n)\ny_censored = pd.DataFrame(\n    dict(\n        event=np.where(\n            censoring_times < y_uncensored[\"duration\"], 0, y_uncensored[\"event\"]\n        ),\n        duration=np.minimum(censoring_times, y_uncensored[\"duration\"]),\n    )\n)\ny_censored[\"event\"].value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_cumulative_incidence_functions(\n    distributions, gb_incidence=gb_incidence, aj=aj, y=y_censored\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the Aalen-Johansen estimator is unbiased and empirically recovers\nthe theoretical curves both with and without censoring. The\nGradientBoostingIncidence estimator also appears unbiased by censoring, but\nthe predicted curves are not smooth and not even monotonically increasing. By\nadjusting the hyper-parameters, notably the learning rate, the number of\nboosting iterations and leaf nodes, it is possible to somewhat control the\nsmoothness of the predicted curves, but it is likely that doing some kind of\nsmoothing post-processing could be beneficial (but maybe at the cost of\nintroducing some bias). This is left as future work.\n\nAlternatively, we could try to enable a monotonicity constraint at training\ntime, however, in practice this often causes a sever over-estimation bias for\nthe large time horizons:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally let's try again to fit the GB Incidence models using a monotonicity\nconstraint:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "monotonic_gb_incidence = clone(gb_incidence).set_params(\n    monotonic_incidence=\"at_training_time\"\n)\nplot_cumulative_incidence_functions(\n    distributions, gb_incidence=monotonic_gb_incidence, y=y_censored\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting incidence curves are indeed monotonic. However, for smaller\ntraining set sizes, the resulting models can be significantly biased, in\nparticular large time horizons, where the CIFs are getting flatter. This\neffect diminishes with larger training set sizes (lower epistemic\nuncertainty).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}